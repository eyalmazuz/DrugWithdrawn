{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, IntervalStrategy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee65bc3",
   "metadata": {},
   "source": [
    "Loading the metrics we'll use to evaluate our model's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3fae6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = evaluate.load(\"roc_auc\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precison = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25daea6",
   "metadata": {},
   "source": [
    "Here we decide with evaluation to use and with dataset to test on\n",
    "\n",
    "In addition, we load the pre-trained model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25fe51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_type = 'db_no_agree_no_dups'\n",
    "dataset_name = 'DrugBank'\n",
    "pretrained_path = \"seyonec/PubChem10M_SMILES_BPE_450k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc75558",
   "metadata": {},
   "source": [
    "here we load the dataset, we use train2 since it's the train file that doesn't contain the validation set insode of it.\n",
    "\n",
    "this ``load_dataset`` method, automatically loads all the files in csv format and creates an HuggingFace's dataset object that is easy to use when fine-tuning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ac8aee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-74e83418e4d2c9a0\n",
      "Found cached dataset csv (/home/eyal/.cache/huggingface/datasets/csv/default-74e83418e4d2c9a0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5a62b96f3e442e89c10f131eb0a561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': f'split/{split_type}/{dataset_name}/train2.csv',\n",
    "                                          'validation': f'split/{split_type}/{dataset_name}/val.csv',\n",
    "                                          'test': f'split/{split_type}/{dataset_name}/test.csv',})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ef7af",
   "metadata": {},
   "source": [
    "removing uncessencary columns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c9b482f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column('withdrawn_class', 'labels').\\\n",
    "            remove_columns(['Unnamed: 0', 'index', 'length', 'inchikey', 'groups', 'source']).\\\n",
    "            with_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426a2ea",
   "metadata": {},
   "source": [
    "here we load our model and tokenizer, we use the ``AutoModel`` and ``AutoTokenizer`` classes as they provide a generic way to load every model in HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e41fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"seyonec/PubChem10M_SMILES_BPE_450k\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/vocab.json\n",
      "loading file merges.txt from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"seyonec/PubChem10M_SMILES_BPE_450k\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"seyonec/PubChem10M_SMILES_BPE_450k\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"seyonec/PubChem10M_SMILES_BPE_450k\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Not Withdrawn\",\n",
      "    \"1\": \"Withdrawn\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Not Withdrawn\": 0,\n",
      "    \"Withdrawn\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/eyal/.cache/huggingface/hub/models--seyonec--PubChem10M_SMILES_BPE_450k/snapshots/c18fccd09b3326bf2d4633412c256d7db872156d/pytorch_model.bin\n",
      "Some weights of the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_path, num_labels=2,\n",
    "                                                           id2label={0: 'Not Withdrawn', 1:'Withdrawn'},\n",
    "                                                           label2id={'Not Withdrawn': 0, 'Withdrawn': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "015dff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"this methods tokenize the smiles into ids which are then fed into the transforemr model\n",
    "    we set the max length of the toknizer to be the longest SMILES in our dataset and pad the rest to this length\"\"\"\n",
    "    return tokenizer(examples[\"smiles\"], padding=\"max_length\", truncation=True, max_length=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee51903c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ce26a4f4644131956950007d0b4ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1322e1030f8e406588c80629bc891fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35576383c81f43f3894d6bd99ae9d697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cb262",
   "metadata": {},
   "source": [
    "a method to compute all the metrics we are using to evaluate our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "927c5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    auc_score = auc.compute(prediction_scores=logits[:, 1], references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels)\n",
    "    aupr = average_precision_score(y_score=logits[:, 1], y_true=labels)\n",
    "    precision_score = precison.compute(predictions=predictions, references=labels)\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels)\n",
    "    return {**f1_score , **{'PR-AUC': aupr}, **accuracy_score, **auc_score, **precision_score, **recall_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d63b47",
   "metadata": {},
   "source": [
    "here we define our entire training arguments\n",
    "this is a simple HuggingFace object that will contain all the parameters we are using in our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12cf250d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{split_type}/{dataset_name}/{pretrained_path}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    save_strategy=IntervalStrategy.STEPS,\n",
    "    report_to='wandb',\n",
    "    run_name=f'{pretrained_path} {split_type} {dataset_name}',\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c621059a",
   "metadata": {},
   "source": [
    "training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccfeaf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset={'Validation': dataset[\"validation\"], 'Test': dataset[\"test\"]},\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4145d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
