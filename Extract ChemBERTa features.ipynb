{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aba80213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaModel\n",
    "from transformers import TrainingArguments, Trainer, IntervalStrategy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3243b67",
   "metadata": {},
   "source": [
    "here we choose the pretained model to extract the transformer embedding from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5a35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = \"seyonec/PubChem10M_SMILES_BPE_450k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c3231",
   "metadata": {},
   "source": [
    "loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b88c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n",
    "model = RobertaModel.from_pretrained(pretrained_path, num_labels=2, add_pooling_layer=True,\n",
    "                                                           id2label={0: 'Not Withdrawn', 1:'Withdrawn'},\n",
    "                                                           label2id={'Not Withdrawn': 0, 'Withdrawn': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516dfd4",
   "metadata": {},
   "source": [
    "this function iterates over all the rows in the dataframe\n",
    "and for each SMILES string it passes it through the model to get the last hidden state\n",
    "the last hidden states are the final representation the model extract for each token in the data before using it for the different classification task\n",
    "\n",
    "it's common in BERT-like models to take the first tokens as the \"pooled\" token that is used to represent the entire string (in the regular BERT model this is the \\[CLS\\] token).\n",
    "\n",
    "then we create a DataFrame each row containing the original SMILES and column for each feature in the pooled token vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe5b35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(df):\n",
    "    embedding_df = pd.DataFrame(columns=['smiles'] + [f'ChemBERTa_emb_{i}' for i in range(768)])\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), leave=False):\n",
    "        encodings = tokenizer(row.smiles, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            output = model(**encodings)\n",
    "            smiles_embeddings = output.last_hidden_state[0, 0, :]\n",
    "\n",
    "        dic = {**{'smiles': row.smiles}, **dict(zip([f'ChemBERTa_emb_{i}' for i in range(768)], output.last_hidden_state[0, 0, :].numpy().tolist()))}\n",
    "\n",
    "        embedding_df.loc[len(embedding_df)] = pd.Series(dic)\n",
    "        \n",
    "    return embedding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40089097",
   "metadata": {},
   "source": [
    "here we iterate over all our data and create embedding for each of the training and testing files\n",
    "and saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0dddaff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3663fa71e448bda73b63cc4cc89967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in tqdm(['db_no_agree_no_dups', 'db_agree_no_dups']):\n",
    "    for dataset in tqdm(['ChEMBL', 'DrugBank', 'NCATS'], leave=False):\n",
    "        df_train = pd.read_csv(f'./split/{split}/{dataset}/train.csv')\n",
    "\n",
    "        df_test = pd.read_csv(f'./split/{split}/{dataset}/test.csv')\n",
    "    \n",
    "        train_embeddings = get_embeddings(df_train)\n",
    "        test_embeddings = get_embeddings(df_test)\n",
    "        \n",
    "        train_embeddings.to_csv(f'./split/{split}/{dataset}/ChemBERTa_embedding_train.csv')\n",
    "        \n",
    "        test_embeddings.to_csv(f'./split/{split}/{dataset}/ChemBERTa_embedding_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190663c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e7b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
